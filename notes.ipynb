{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d8b2b7",
   "metadata": {},
   "source": [
    "# An Introduction to LangChain and Agentic AI\n",
    "\n",
    "## LLM overview \n",
    "- What is an LLM? How does it work? \n",
    "- Prompts & controlling a model (system prompt, user prompt, strategies like few-shot prompting)\n",
    "   - prompt with: available tools, output schema, safety constraints (e.g., what \"shouln't\" our model do)\n",
    "\n",
    "## What are chains and agents? \n",
    "- Chain: fixed recipe - pre-designed sequence of steps. \n",
    "  - Decide ahead of time how this flow works, there is no branching or decision-making by the model. \n",
    "  - RAG: \n",
    "    1. Split a long doc into chunks \n",
    "    2. Compute embeddings \n",
    "    3. For a query, we compute the top-k most relevant chunks \n",
    "    4. Feed the chunks and query to the LLM \n",
    "    5. Produce some summary or answer \n",
    "  - We know in advance exactly which tools get used, and in what order. \n",
    "- (chef) Agent: run-time decision-making. ( not just answer, but auto act accordingly like send email for us ) \n",
    "  - more flexible \n",
    "  - model that decides what tool to use based on what it has learnt so far \n",
    "  - reason -> act (e.g., call a tool) -> observation (model sees observation from tool) -> reasons again (loop) \n",
    "\n",
    "**ReAct loop:** \n",
    "1. (User Question -> ) Reason/ Plan - given what we know, what is the next best action? \n",
    "2. Act - call a tool/functions (search, wikipedia lookup, calculator, retrieving content from documents, external API calls to get and give answer), or even answer now \n",
    "3. Observe - read the tool's output ( -> Reason/ Update and Return answer)\n",
    "4. Repeat \n",
    "\n",
    "( (recipe) Chained fixed sequence: User Question -> Retrieve relevant tool -> Feed to model - prompt -> Final Answer )\n",
    "\n",
    "**RAG integration:**\n",
    "- LLMs have limitations - training data might be outdated, or they might not know niche topics.\n",
    "- Precise *factual* detail -> often hallucinates.\n",
    "- Models don't know our private information.\n",
    "- We could put this information into the prompt - it doesn't scale. \n",
    "\n",
    "- RAG := retrieval-augmented generation\n",
    "  - 1. Query arrives\n",
    "  - 2. System retrieves from a vector index the top-k relevant document chunks\n",
    "  - 3. Retrieved text is \"context injected\" into the prompt to the model.\n",
    "  - 4. Model generated an answer *grounded* in the retrieved text. \n",
    "\n",
    "- => less likely to hallucinate\n",
    "- => answer questions about private data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
